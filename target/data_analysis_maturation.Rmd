---
title: "Data analysis: maturation"
author: "Mar Garcia-Aloy"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    toc: true
    number_sections: false
    toc_float: true
---

```{r startpoint, include = FALSE}
startpoint <- Sys.time()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

This document belongs to the project of application of the new targeted lipidomics method to grape samples collected at different maturation stages.  

## Libraries

```{r}
library(readxl)
library(xcms)
library(plotly)
library(pheatmap)
library(RColorBrewer)
library(knitr)
library(car)
library(mgcv)
library(beeswarm)
```


# Read data

```{r}
# Import compound quantification data:
data <- read_xlsx("data/data_maturation.xlsx", sheet = "trans_data")
colnames(data)[1] <- "ID"

# Create a vector specifying the sample classes:
class <- colnames(data)[-1]
class <- gsub("_Rep.*", "", class)
class <- gsub("_MIX.*", "", class)
class[grep("QC", class)] <- "QC"

# Load compound annotations:
fl <- "C:/Users/garciaalom/Google Drive/projectes/lipidomics_shared/new_rt.xlsx"
cmps <- rbind(read_xlsx(fl, sheet = "IS")[, 1:2],
              read_xlsx(fl, sheet = "FORMULE")[, 1:2])
cmps$ID <- gsub("_Na", "", cmps$ID)
cmps <- cmps[cmps$ID %in% data$ID, ]
cmps <- cmps[match(data$ID, cmps$ID), ]

# Replace R problematic symbols in compound names:
all(cmps$ID == data$ID) # TRUE
cmps$ID <- data$ID <- gsub("[^0-9A-Za-z///' ]", "_", cmps$ID)
cmps$ID <- data$ID <- gsub("/", "_", cmps$ID)
cmps$ID <- data$ID <- gsub(" ", "_", cmps$ID)
cmps$ID <- data$ID <- gsub("__", "_", cmps$ID)
idx <- grep("_$", cmps$ID)
cmps$ID[idx] <- data$ID[idx] <- substr(cmps$ID[idx], 1, nchar(cmps$ID[idx])-1)
data <- data[,-1]
data <- sapply(data, as.numeric)
rownames(data) <- cmps$ID
```

Below we import and plot the data regarding the brix degree of collected samples according to the sampling date.  

```{r}
brix <- readxl::read_excel("data/Grape_Brix_Datafile.xlsx")
plot(as.Date(brix$data), brix$Brix, 
     col = brix$Replicate + 1, pch = 16, xlab = "Date", ylab = "Brix")
legend("bottomright", legend = paste("Rep ", seq(5)), col = 2:6, pch = 16)
```

The dataset contains information about the quantification of n=`r nrow(data)` compounds in n=`r ncol(data)` samples of the following classes: `r paste0(aggregate(class, list(class), function(x){sum(class == x)})[,1], " (n=", aggregate(class, list(class), function(x){sum(class == x)})[,2], ")", collapse = "; ")`. 

Missing values (n=`r sum(is.na(data))`) in the dataset will be replaced by a random value between the range obtained by dividing by 1000 and 100 of the smallest measured concentration for the corresponding compound.    
Principal component analysis (PCA) and heatmaps will be performed on log-transformed and Pareto-scaled abundances, whereas one-way analysis of variance (ANOVA) on log-transformed data.  


# Exploratory data analysis

As mentioned, we start with an exploratory analysis of the data, first using all compounds and all samples and then only with samples of interest and excluding some potential problematic compounds.


## All samples

### PCA

```{r}
scaling.pareto <- BioMark::scalefun(sc.p="pareto")

set.seed(123)
dt <- t(imputeRowMinRand(as.matrix(data), method = "from_to",
                         min_fraction = 1/100,
                         min_fraction_from = 1/1000))
colnames(dt) <- rownames(data)
dt <- log10(dt)
dt <- data.frame(scaling.pareto(dt))
pca <- prcomp(dt, center = FALSE, scale. = FALSE)
tmp <- data.frame(pca$x)
plot_ly(x = tmp$PC1, y = tmp$PC2,
        text = rownames(tmp),
        color = class)
```

`QC` samples are clustered together close to the origin of the scores plot. They show a lower dispersion between them compared to that observed among the study samples and also among the samples from the same study group (i.e., time-point), indicating a good analytical reproducibility (i.e., the variability due to the analytical platform is smaller than the biological/technical variability between the samples, eiher the ones collected at the same moment).  

Visual inspection of the PCA score shows a certain degree of separation between the first and second half of samples along the first two components (accounting for `r round(summary(pca)$importance[2,1]*100, 1)`% and `r round(summary(pca)$importance[2,2]*100, 1)`% of the total variance explained, respectively).  
To visualise this more clearly, the samples below are coloured according to whether they belong to the first or the second half of sampling:


```{r}
class2 <- class
class2[class2 %in% c("Pt_01", "Pt_02", "Pt_03", "Pt_04", "Pt_05", 
                     "Pt_06")] <- "Pt_01_06"
class2[class2 %in% c("Pt_07", "Pt_08", "Pt_09", "Pt_10", "Pt_11", 
                     "Pt_12", "Pt_13")] <- "Pt_07_13"
plot_ly(x = tmp$PC1, y = tmp$PC2,
        text = rownames(tmp),
        color = class2)


tmp <- data.frame(pca$rotation)
plot_ly(x = tmp$PC1, y = tmp$PC2,
        text = rownames(tmp),
        color = cmps$class)
```

The distribution of the compounds along the first 2 components in the loading plot does not match the same pattern observed in the samples: while in the case of samples a separation according to the sampling time is observed along a diagonal that goes from the upper-right corner to the lower-left corner, the compounds seem to be distributed along a diagonal that goes from the upper-left corner to the lower-right corner.   
However, it can be clearly observed how the `TAGs` are located outside this diagonal in the lower-left quadrant of the graph, although *a priori* it is not easy to associate them to a certain group of samples.


### Heatmap

```{r, fig.height=10, fig.width=14}
dt_pt <- data.frame(point = class)
rownames(dt_pt) <- rownames(dt)

all(cmps$ID == rownames(data)) # TRUE
dt_class <- data.frame(class = cmps$class)
rownames(dt_class) <- colnames(dt)

n.pt <- length(unique(dt_pt$point))
n.class <- length(unique(dt_class$class))

ann_color <- list(
  "time-point" = colorRampPalette(brewer.pal(9, "Set1"))(n.pt),
  "class" = colorRampPalette(brewer.pal(12, "Paired"))(n.class)
)
names(ann_color[[1]]) <- unique(dt_pt$point)
names(ann_color[[2]]) <- unique(dt_class$class)

pheatmap(dt, show_colnames = F, show_rownames = F, 
         annotation_row = dt_pt, annotation_col = dt_class,
         annotation_colors = ann_color)


dt_pt2 <- data.frame(point = class2)
rownames(dt_pt2) <- rownames(dt)

ann_color <- list(
  "time-point" = brewer.pal(3, "Set2"),
  "class" = colorRampPalette(brewer.pal(12, "Paired"))(n.class)
)
names(ann_color[[1]]) <- unique(dt_pt2$point)
names(ann_color[[2]]) <- unique(dt_class$class)

pheatmap(dt, show_colnames = F, show_rownames = F, 
         annotation_row = dt_pt2, annotation_col = dt_class,
         annotation_colors = ann_color)
```

## Quality control samples

We check (and, if any, list) if there are compounds not detected in all QC samples:


```{r}
idx <- which(rowSums(is.na(data[, class == "QC"])) > 0)
tmp <- data[idx, class == "QC"]
rownames(tmp) <- rownames(data)[idx]
kable(tmp)
```

Next we calculate the CV across QC samples to check the quality of acquired data and their distribution is plotted below:

```{r}
CV <- function(x){(sd(x)/mean(x))*100}
CV_QC <- apply(data[, class == "QC"], 1, CV)
plot(density(CV_QC, na.rm = TRUE), xlab = "CV (%)", main = "")
```

The CV is arround or below 10% for most of the compounds, but some have a higher CV.  
The following table shows the the amount of compounds with specific CV intervals:  

```{r}
kable(table(cut(CV_QC, c(0, 10, 20, 30, 50, 80, max(CV_QC, na.rm = T)))))
```

Below it is plotted the distribution within QC samples of the 4 compounds with the highest CV in QCs:

```{r, fig.height = 7}
par(mfrow = c(2, 2))
idx <- order(CV_QC, decreasing = T)[1:4]
for(i in seq(4)){
  plot(data[idx[i], class == "QC"], xlab = "", ylab = "", pch = 16, 
       main = paste0(rownames(data)[idx][i], " (CV=", round(CV_QC[idx[i]]), "%)"))
}
```

We still have to decide if to exclude some compounds based what it has been observed within QC samples.


## Study samples

Below, we will repeat the unsupervised exploratory data anlyses, but this time excluding the `QC` samples, and `IS` compounds and potentially problematic analytes.

```{r}
class_st <- class[class != "QC"]

data_st <- data[, class != "QC"]
rownames(data_st) <- rownames(data)

data_st <- data_st[cmps$class != "IS", ]
cmps <- cmps[cmps$class != "IS",]
rownames(data_st) <- cmps$ID

na_n <- apply(data_st, 1, function(x) length(which(is.na(x))))
table(na_n)

idx <- which(na_n > 30)
kable(t(data_st[idx,]))

na_n <- aggregate(t(data_st), by=list(class_st), function(x){sum(!is.na(x))})
na_n <- apply(na_n[,-1], 2, max)
head(na_n[order(na_n)])
```

There is n=`r sum(na_n == 1)` compound (`r names(which(na_n == 1))`) that, although it has detected in >1 sample, it has only been detected at maximum in 1 sample sample within a same class.  
In parallel, there are n=`r sum(na_n > 1 & na_n < 5)` other compounds (`r paste(names(which(na_n > 1 & na_n < 5)), collapse = " & ")`) that at maximum have been detected in 2 and 3 samples, respectively, within the sample class.  
We still have to decide if to exclude some or all of these compounds.  


### PCA

```{r}
set.seed(123)
dt <- t(imputeRowMinRand(as.matrix(data_st), method = "from_to",
                         min_fraction = 1/100,
                         min_fraction_from = 1/1000))
dt <- log10(dt)
dt <- data.frame(scaling.pareto(dt))
pca <- prcomp(dt, center = FALSE, scale. = FALSE)
tmp <- data.frame(pca$x)
plot_ly(x = tmp$PC1, y = tmp$PC2,
        text = rownames(tmp),
        color = class_st)

all(cmps$ID == rownames(data_st)) # TRUE
tmp <- data.frame(pca$rotation)
plot_ly(x = tmp$PC1, y = tmp$PC2,
        text = rownames(tmp),
        color = cmps$class)
```


### Heatmap

```{r}
dt_pt2 <- data.frame(point = class2[class != "QC"])
rownames(dt_pt2) <- rownames(dt)

ann_color <- list(
  "time-point" = brewer.pal(3, "Set2"),
  "class" = colorRampPalette(brewer.pal(12, "Paired"))(n.class)
)
names(ann_color[[1]]) <- unique(dt_pt2$point)
names(ann_color[[2]]) <- unique(dt_class$class)

pheatmap(dt, show_colnames = F, show_rownames = F, 
         annotation_row = dt_pt2, annotation_col = dt_class,
         annotation_colors = ann_color)
```


# Differential abundance analysis

Generalised Additive Models (GAMs) are performed to identify those compounds that show specific trends over time. GAMs have been selected since they also catch those trends that do not fit a simple linear model.  
GAMs are regression models which allow for the inclusion of a non-parametric smoothing and will fit a regression spline to the data, allowing for nonlinear relationships.   

Sources:   

- http://environmentalcomputing.net/intro-to-gams/  
- https://www.nature.com/articles/hortres201738  

```{r}
set.seed(123)
dt <- t(imputeRowMinRand(as.matrix(data_st), method = "from_to",
                         min_fraction = 1/100,
                         min_fraction_from = 1/1000))
```


## Assumtions

### Homogeneity of variance

Below we apply the function `leveneTest()` from `car` package to the log-transformed data to check the homogeneity of variances across the time-defined groups using the **Levene's test**.  

```{r}
stat.levene <- function(x){
  unlist(leveneTest(log10(x) ~ factor(class_st)))["Pr(>F)1"]
}
lev <- apply(FUN = stat.levene, MARGIN = 2, X = dt)
hist(lev, breaks = 32)
summary(lev)
```

For n=`r sum(lev < 0.05)` out of `r ncol(dt)` (`r round((sum(lev < 0.05) / ncol(dt))*100, 1)`%) compounds, the homoscedasticity assumption is violated. 


### Normality

The **Shapiro-Wilk test** on the GAM residuals is used to check if the residuals satisfy the homoscedasticity assumption. 

```{r}
brix <- rbind(brix, 
              c(2019, "diradamento", "corno - az. Oliva", "2019-09-17", "C", 0, 
                mean(brix$Brix[brix$data == "2019-09-17"])),
              c(2019, "diradamento", "corno - az. Oliva", "2019-09-24", "C", 0, 
                mean(brix$Brix[brix$data == "2019-09-24"])))
brix <- brix[order(brix$data, brix$Replicate), ]

stat.shap <- function(x){
  as.numeric(unlist(shapiro.test(residuals.gam(
    gam(log10(x) ~ s(as.integer(as.Date(brix$data))), method = "REML")
  )))["p.value"])
}
shap <- apply(FUN = stat.shap, MARGIN = 2, X = dt)
hist(shap, breaks = 32)
summary(shap)
```

In this case, it can be seen that for a total of `r sum(shap < 0.05)` out of `r ncol(dt)` (`r round((sum(shap < 0.05) / ncol(dt))*100, 1)`%) compounds, the normality assumption is violated.   


## Compute the test 

GAMs are fitted by using the `gam` function of the `mgcv` package on log-transformed data.  
GAMs are fitted by using the restricted maximum likelihood (REML) algorithm because REML estimates are more nearly unbiased in presence of small data sets.  
Raw p-values are adjusted for multiple hypothesis testing with the method from Benjamini and Hochberg.  
Analytes will be considered significant if they have an adjusted p-value smaller than 0.05 (representing a 5% false discovery rate).  

```{r}
stat.gam <- function(x){
  as.numeric(unlist(summary(
    gam(log10(x) ~ s(as.integer(as.Date(brix$data))), method = "REML")
  ))["s.table4"])
}
pval <- apply(FUN = stat.gam, MARGIN = 2, X = dt)
padj <- p.adjust(pval, "BH")
par(mfrow = c(1, 2))
hist(pval, breaks = 32)
hist(padj, breaks = 32)
```

There is a clear enrichment of small p-values.  

## Some specific examples

```{r, fig.height = 10}
idx <- which(colnames(dt) %in% c("PG_14_5_18_3", "PG_16_0_18_1", "PG_14_1_20_2"))
col_group <- colorRampPalette(brewer.pal(9, "Set1"))(13)
names(col_group) <- unique(class_st)

par(mfrow = c(3, 3))
for(i in seq(3)){
  vals <- split(dt[,idx[i]], f = class_st)
  beeswarm(vals, col = col_group[names(vals)], pch = 16, main = colnames(dt)[idx[i]])
  bxplot(vals, probs = 0.5, col = "#00000060", add = TRUE)
  
  vals <- split(log10(dt[,idx[i]]), f = class_st)
  beeswarm(vals, col = col_group[names(vals)], pch = 16, main = "log-data")
  bxplot(vals, probs = 0.5, col = "#00000060", add = TRUE)
  
  
  gam_i <- gam(log10(dt[,idx[i]]) ~ s(as.integer(as.Date(brix$data))), method = "REML")
  plot(gam_i)
  print(colnames(dt)[idx[i]])
  print(summary(gam_i))
}
```


## Discriminant compounds

A total of n=`r sum(padj < 0.05)` compounds (`r round((sum(padj < 0.05)/nrow(data_st))*100, 1)`%) are found to present significant time-related trends.   

Below there is plotted again the loadings plot of the PCA and the heatmap highlighting the significant compounds.

```{r}
tmp <- data.frame(pca$x)
plot_ly(x = tmp$PC1, y = tmp$PC2,
        text = rownames(tmp),
        color = class2[class != "QC"])

tmp <- data.frame(pca$rotation)
plot_ly(x = tmp$PC1, y = tmp$PC2,
        text = rownames(tmp),
        color = (padj < 0.05), colors = c("#377EB8", "#E41A1C"))

dt_sig <- data.frame(significant = (padj < 0.05))
dt_sig$significant <- as.factor(dt_sig$significant)
rownames(dt_sig) <- colnames(dt)
ann_color <- list(
  "time-point" = brewer.pal(5, "Set1")[2:3],
  "significant" = c("#377EB8", "#E41A1C")
)
names(ann_color[[1]]) <- unique(dt_pt2$point)
names(ann_color[[2]]) <- c("FALSE", "TRUE")
pheatmap(data.frame(scaling.pareto(log10(dt))), 
         annotation_row = dt_pt2, annotation_col = dt_sig,
         annotation_colors = ann_color, show_colnames = F, show_rownames = F)
```

The time-trends for each metabolite are established according to Pearson-correlation values:

```{r, eval=FALSE}
tmp <- apply(dt, 2, function(x){cor(log10(x), as.integer(as.Date(brix$data)))})
head(tmp[padj < 0.05][order(tmp[padj < 0.05])])
tail(tmp[padj < 0.05][order(tmp[padj < 0.05])])
tmp[which(names(tmp) == "PG_14_1_20_2")]
cor(log10(dt[1:30,207]), as.integer(as.Date(brix$data[1:30])))
cor(log10(dt[31:63,207]), as.integer(as.Date(brix$data[31:63])))

tmp1 <- apply(dt, 2, function(x){cor(log10(x[1:30]), as.integer(as.Date(brix$data[1:30])))})
tmp2 <- apply(dt, 2, function(x){cor(log10(x[31:63]), as.integer(as.Date(brix$data[31:63])))})

cmps$beh <- "X"
cmps$beh[tmp1 > 0.5 & tmp2 < (-0.5)] <- "up-down"
cmps$beh[tmp1 < (-0.5) & tmp2 > 0.5] <- "down-up"
cmps$beh[tmp > 0.5] <- "up"
cmps$beh[tmp < (-0.5)] <- "down"
table(cmps$beh, padj < 0.05)

which(cmps$beh == "X" & padj < 0.05)[1]
```


## Correlation with predicted values

```{r}
xtime <- as.integer(as.Date(brix$data))
gam.cor.pred <- function(x){
  cor(log10(x), 
      predict(gam(log10(x) ~ s(xtime), method = "REML"), 
              data.frame(xtime = xtime)))
}

cors <- apply(FUN = gam.cor.pred, MARGIN = 2, X = dt)

tmp <- cors[padj < 0.05]
tmp <- tmp[order(tmp, decreasing = T)]

par(mfrow =c(1, 3))
for(i in seq(length(tmp))){
  y <- dt[,names(tmp[i])]
  gam_i <- gam(log10(y) ~ s(xtime), method = "REML")
  plot(gam_i, main = paste0(names(tmp[i]), " (cor=", round(tmp[i], 3), ")"))
}
```


# Session information

```{r session}
Sys.time()-startpoint
devtools::session_info()
```
